#!/usr/bin/env python
import sys
import os
import optparse
import traceback
import operator
import certifi
import re
import json
from datetime import datetime,timedelta
from elasticsearch import Elasticsearch
from elasticsearch_dsl import Q,A, Search

import TextUtils
import Configuration
from TimeUtils import TimeUtils
import NiceNum
from MySQLUtils import MySQLUtils
from Reporter import Reporter
from indexpattern import indexpattern_generate


class User:
    def __init__(self, info):
        """Take CSV as described below and assigns to it the attributes vo, facility, email, user, hours, eff 
        # CSV format DARKSIDE, Fermigrid, /CN = fifegrid/CN = batch/CN = Shawn S. Westerdale/CN = UID:shawest, 
        # 13411.2019444, 0.969314375191
        # New CSV format 'uboone', 'GPGrid', '/CN=fifegrid/CN=batch/CN=Elena Gramellini/CN=UID:elenag', '1337.86666667', '0.857747616437'        
        """
        tmp = info.split(',')
        self.vo = tmp[0].lower()
        self.facility = tmp[1]
        self.email, self.user = self.parseCN(tmp[2])
        self.hours = int(float(tmp[3]))
        self.eff = round(float(tmp[4]), 2)

    def parseCN(self, cn):
        """Parse the CN to grab the email address and user"""
        indx = cn.find("/CN=UID:")
        if indx > 0:
            email = '{}@fnal.gov'.format(cn[indx + len("/CN=UID:"):])
        else:
            email = ""
            indx = cn.rfind('/')
        user = cn[cn[:indx].rfind("=") + 1:indx]
        return email, user

    def dump(self):
        print "{:>10}, {:>20}, {:>20}, {}, {}".format(self.vo, self.facility, self.user, int(self.hours), round(self.eff, 2))


class Efficiency(Reporter):
    def __init__(self, config, start, end, verbose, hour_limit, eff_limit, users, isTest):
        Reporter.__init__(self, config, start, end, verbose)
        self.hour_limit = hour_limit
        self.eff_limit = eff_limit
        self.users = users
        self.isTest = isTest

    def reportVO(self, vo, facility):
        """Method to generate report for VO from users dictionary"""
        if vo == "FIFE":
            records = [rec for rec in self.users.values()]
        else:
            records = self.users[vo.lower()]
        info = [rec for rec in records if ((rec.hours > self.hour_limit and rec.eff < self.eff_limit) and (facility == "all" or rec.facility == facility))]
        return sorted(info, key=lambda user: user.eff)

    def sendReport(self, vo, report):
        """Generate HTML from report and send the email"""
        if len(report) == 0:
            print "Report empty"
            return
        
        table = ""
        for u in report:
            table += '<tr><td align="left">{}</td><td align="left">{}</td>'.format(u.vo.upper(), u.facility) + \
                     '<td align="left">{}</td><td align="right">{}</td><td align="right">{}</td></tr>'.format(
                                                                        u.user, NiceNum.niceNum(u.hours), u.eff)
        
        text = "".join(open("template_efficiency.html").readlines())
        text = text.replace("$START", self.start_time)
        text = text.replace("$END", self.end_time)
        text = text.replace("$TABLE", table)
        text = text.replace("$VO", vo.upper())
        
        if self.verbose:
            fn = "{}-efficiency.{}".format(vo.lower(), self.start_time.replace("/", "-"))
            with open(fn,'w') as f:
                f.write(text)
        
        if self.isTest:
            emails = self.config.get("email", "test_to").split(",")
        else:
            emails = self.config.get(vo.lower(), "email").split(",") + self.config.get("email", "test_to").split(",")
        TextUtils.sendEmail(
                            ([], emails), 
                            "{} Jobs with Low Efficiency ({})  on the  OSG Sites ({} - {})".format(
                                                                                                    vo, 
                                                                                                    self.eff_limit, 
                                                                                                    self.start_time, 
                                                                                                    self.end_time), 
                            {"html": text},
                            ("Gratia Operation", "sbhat@fnal.gov"),
                            "smtp.fnal.gov")
        
        if self.verbose:
            os.remove(fn)
        
        print "Report sent"


def es_efficiency_to_file(start_time, end_time, vo, verbose = False):
    """Function that queries elasticsearch (GRACC) for data.  Returns the csv file with aggregated data"""
    outfile = 'efficiency.csv'
    # Header
    header = '{}\t{}\t{}\t{}\t{}\n'.format('VO',
                                         'Host Description',
                                         'Common Name',
                                         'Wall Hours',
                                         'Efficiency'
                                         )

    with open(outfile,'w') as f:
        f.write(header)

    client=Elasticsearch(['https://gracc.opensciencegrid.org/e'],
    #client=Elasticsearch(['https://fifemon-es.fnal.gov'],
                         use_ssl = True,
                         verify_certs = True,
                         ca_certs = certifi.where(),
                         client_cert = 'gracc_cert/gracc-reports-dev.crt',
                         client_key = 'gracc_cert/gracc-reports-dev.key',
                         timeout = 60)

    start_date = re.split('[-/ :]', start_time)
    starttimeq = datetime(*[int(elt) for elt in start_date]).isoformat()

    end_date = re.split('[-/ :]', end_time)
    endtimeq = (datetime(*[int(elt) for elt in end_date])+timedelta(days=1)).isoformat()

    wildcardVOq = '*'+vo+'*'
    wildcardProbeNameq = 'condor:fifebatch?.fnal.gov'


    s = Search(using = client,index = indexpattern_generate(start_date,end_date))\
               .query("wildcard",VOName=wildcardVOq)\
               .query("wildcard",ProbeName=wildcardProbeNameq)\
               .filter("range",EndTime={"gte":starttimeq,"lt":endtimeq})\
               .filter(Q({"range":{"WallDuration":{"gt":0}}}))\
               .filter(Q({"term":{"Host_description":"GPGrid"}}))\
               .filter(Q({"term":{"ResourceType":"Payload"}}))\
               [0:0]       #Size 0 to return only aggregations
           
    Bucket = s.aggs.bucket('group_VOname','terms',field='ReportableVOName')\
            .bucket('group_HostDescription','terms',field='Host_description')\
            .bucket('group_commonName','terms',field='CommonName')

    Metric = Bucket.metric('Process_times_WallDur','sum',script="(doc['WallDuration'].value*doc['Processors'].value)")\
            .metric('WallHours','sum',script="(doc['WallDuration'].value*doc['Processors'].value)/3600")\
            .metric('CPUDuration','sum',field='CpuDuration')
    #Pipeline = Metric.pipeline('Test','bucket_script',buckets_path=['CPUDuration','WallHours'],script='CPUDuration/WallHours')  #Right now, failing because Processors isn't numeric.  Follow up with Kevin.  Up until here, it works
 
    if verbose:
        t = s.to_dict()
        print json.dumps(t,sort_keys=True,indent=4)

    response = s.execute()
    resultset = response.aggregations

    if verbose:
        print json.dumps(response.to_dict(),sort_keys=True,indent=4)
    
    with open(outfile,'a') as f:
        for per_vo in resultset.group_VOname.buckets:
            for per_hostdesc in per_vo.group_HostDescription.buckets:
                for per_CN in per_hostdesc.group_commonName.buckets:
                    if per_CN.WallHours.value > 1000:  #Value from config file
                        outstring = '{},{},{},{},{}\n'.format(vo,
                                                                  per_hostdesc.key,
                                                                  per_CN.key,
                                                                  per_CN.WallHours.value,
                                                                  (per_CN.CPUDuration.value/3600) / per_CN.WallHours.value
                                                                  )
                        f.write(outstring)

    return outfile


def parse_opts():
    """Parses command line options"""
    usage = "Usage: %prog [options]"
    parser = optparse.OptionParser(usage)
    parser.add_option("-c", "--config", dest="config", type="string",
                      help="report configuration file (required)")
    parser.add_option("-v", "--verbose",
                      action="store_true", dest="verbose", default=False,
                      help="print debug messages to stdout")
    parser.add_option("-E", "--experiement",
                      dest="vo", type="string",
                      help="experiment name")
    parser.add_option("-F", "--facility",
                      dest="facility", type="string",
                      help="facility  name")
    parser.add_option("-s", "--start", type="string",
                      dest="start", help="report start date YYYY/MM/DD HH:MM:DD or YYYY-MM-DD HH:MM:DD (required)")
    parser.add_option("-e", "--end", type="string",
                      dest="end", help="report end date YYYY/MM/DD HH:MM:DD or YYYY-MM-DD HH:MM:DD")
    parser.add_option("-d", "--dryrun", action="store_true", dest="isTest", default=False,
                      help="send emails only to testers")

    opts, args = parser.parse_args()
    Configuration.checkRequiredArguments(opts, parser)
    return opts, args


if __name__ == "__main__":
    opts, args = parse_opts()
    try:
       # Set up the configuration 
        config = Configuration.Configuration()
        config.configure(opts.config)
       # Grab VO
        vo = opts.vo
       # Grab the limits
        eff = config.config.get(opts.vo.lower(), "efficiency")
        min_hours = config.config.get(opts.vo.lower(), "min_hours")
       
        
    #This is the area that generates the CSV file.  Replace it with a function that generates the CSV
       #command = config.config.get("query", "curl")
       # command = command.replace("$START", opts.start.split()[0])
       # command = command.replace("$END", opts.end.split()[0])
       # command = command.replace("$VO", opts.vo.lower())
       # command = command.replace("$MIN_HOURS", min_hours)
       # MySQLUtils.executeCmd("curl %s > efficiency.csv" % (command,), True)

        # Run our elasticsearch query, get results as CSV 
        resultfile = es_efficiency_to_file(opts.start, opts.end, opts.vo, opts.verbose)

        with open(resultfile,'r') as file:
            f = file.readlines()
        users = {}
        for line in f[1:]:
            u = User(line)
            #u = User(line[:-1].replace("(", '').replace(")", '').replace("'", ""))
            if not users.has_key(u.vo):
                users[u.vo] = []
            users[u.vo].append(u)
        e = Efficiency(config, opts.start, opts.end, opts.verbose, int(min_hours), float(eff), users, opts.isTest)
        if vo == "FIFE" or users.has_key(vo.lower()):
            r = e.reportVO(vo, opts.facility)
            e.sendReport(vo, r)
    except:
        print >> sys.stderr, traceback.format_exc()
        sys.exit(1)
    sys.exit(0)
